{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "看代码差不多看懂了，我在这里大概解释一下吧：\n",
    "soft triple loss其实就是把每一个class分成了k个center，在这里我们设k=10，通过将data分类到centers上而不是class上以增强分类的准确率，\n",
    "个人感觉这里有点像boost之类的操作。。。soft triple的代码把训练分成两部分，一部分是backbone的神经网络，另一部分是一个全连接（dim，cN*K），\n",
    "这里dim是backbone出来的数据维数（embedding的维度），cN是class number， K是center数量。\n",
    "\n",
    "源代码里面backbone的训练用的是lr=0.0001的adam，而fc的训练用的是lr=0.01的adam，这里论文如果没有详细解释的话我个人认为有特意调参的痕迹\n",
    "\n",
    "因为few shot的代码结构实在有点离谱，所以我只改了baseline+soft的网络，用soft triple的参数0.0001/0.01效果比baseline要差很多，你可以试试别的参数\n",
    "参数的设置在io_utils中，当然你也可以用 --modellr和--centerlr修改\n",
    "使用soft triple 的命令是 --method baseline_soft\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# io_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args(script):\n",
    "    parser.add_argument('--K'           , default=10,           help='num of centers for each class')\n",
    "    parser.add_argument('--la'          , default=20, type=float,  help='lambda of soft triple')\n",
    "    parser.add_argument('--gamma'       , default=0.1, type=float, help='gamma of soft triple')\n",
    "    parser.add_argument('--tau'         , default=0.2, type=float,  help='tau of soft triple')\n",
    "    parser.add_argument('--margin'      , default=0.01, type=float, help='margin of soft triple')\n",
    "    parser.add_argument('--modellr', default=0.0001, type=float, help='model learning rate of soft triple')\n",
    "    parser.add_argument('--centerlr', default=0.01, type=float, help='center learning rate of soft triple')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods.baselinetrain_softtriple import BaselineTrainSoft\n",
    "\n",
    "if params.method == 'baseline_soft':\n",
    "    optimizer = torch.optim.Adam([{\"params\": model.feature.parameters(), \"lr\": params.modellr},\n",
    "                                      {\"params\": model.fc, \"lr\": params.centerlr}]) #line 28-30\n",
    "\n",
    "elif params.method =='baseline_soft'\n",
    "    model           = BaselineTrainSoft( model_dict[params.model], la = params.la, gamma = params.gamma, \n",
    "                                        tau=params.tau, margin=params.margin, K=params.K, cN=params.num_classes)# line115-116"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save_features.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods.baselinetrain_softtriple import BaselineTrainSoft #line 13\n",
    "\n",
    "if not params.method in ['baseline', 'baseline++', 'baseline_soft'] : #line77"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods.baselinetrain_softtriple import BaselineTrainSoft #line 19\n",
    "\n",
    "    if params.method == ('baseline' or 'baseline_soft'):# line 61"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# methods.baselinetrain_softtriple.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "This is a new file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "m = 'b_l'\n",
    "\n",
    "if (m =='b') or (m=='b_l'):\n",
    "    print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(base_loader, val_loader, model, optimization, start_epoch, stop_epoch, params):    \n",
    "    if optimization == 'Adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters())\n",
    "    else:\n",
    "        raise ValueError('Unknown optimization, please define by yourself')\n",
    "\n",
    "    max_acc = 0       \n",
    "\n",
    "    for epoch in range(start_epoch,stop_epoch):\n",
    "        model.train()\n",
    "        model.train_loop(epoch, base_loader,  optimizer ) #model are called by reference, no need to return \n",
    "        model.eval()\n",
    "\n",
    "        if not os.path.isdir(params.checkpoint_dir):\n",
    "            os.makedirs(params.checkpoint_dir)\n",
    "\n",
    "        acc = model.test_loop( val_loader)\n",
    "        if acc > max_acc : #for baseline and baseline++, we don't use validation in default and we let acc = -1, but we allow options to validate with DB index\n",
    "            print(\"best model! save...\")\n",
    "            max_acc = acc\n",
    "            outfile = os.path.join(params.checkpoint_dir, 'best_model.tar')\n",
    "            torch.save({'epoch':epoch, 'state':model.state_dict()}, outfile)\n",
    "\n",
    "        if (epoch % params.save_freq==0) or (epoch==stop_epoch-1):\n",
    "            outfile = os.path.join(params.checkpoint_dir, '{:d}.tar'.format(epoch))\n",
    "            torch.save({'epoch':epoch, 'state':model.state_dict()}, outfile)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import configs\n",
    "import backbone\n",
    "from data.datamgr import SimpleDataManager, SetDataManager\n",
    "from methods.baselinetrain import BaselineTrain\n",
    "from methods.baselinefinetune import BaselineFinetune\n",
    "from methods.protonet import ProtoNet\n",
    "from methods.matchingnet import MatchingNet\n",
    "from methods.relationnet import RelationNet\n",
    "from methods.maml import MAML\n",
    "#from methods.baselinetrain_softtriple import BaselineTrainSoft\n",
    "from io_utils import model_dict, parse_args, get_resume_file  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import backbone\n",
    "import utils\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "\n",
    "class BaselineTrainSoft(nn.Module):\n",
    "    def __init__(self, model_func, la, gamma, tau, margin, cN, K):\n",
    "        super(BaselineTrainSoft, self).__init__()\n",
    "        self.feature = model_func()\n",
    "        self.la = la\n",
    "        self.gamma = 1. / gamma\n",
    "        self.tau = tau\n",
    "        self.margin = margin\n",
    "        self.cN = cN\n",
    "        self.K = K\n",
    "        self.fc = Parameter(torch.Tensor(self.feature.final_feat_dim, cN * K))\n",
    "        self.weight = torch.zeros(cN * K, cN * K, dtype=torch.bool)\n",
    "        for i in range(0, cN):\n",
    "            for j in range(0, K):\n",
    "                self.weight[i * K + j, i * K + j + 1:(i + 1) * K] = 1\n",
    "        init.kaiming_uniform_(self.fc, a=math.sqrt(5))\n",
    "        self.DBval = False;  # only set True for CUB dataset, see issue #31\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = Variable(x)\n",
    "        out = self.feature.forward(x)\n",
    "        centers = F.normalize(self.fc, p=2, dim=0)\n",
    "        simInd = out.matmul(centers)\n",
    "        simStruc = simInd.reshape(-1, self.cN, self.K)\n",
    "        prob = F.softmax(simStruc * self.gamma, dim=2)\n",
    "        simClass = torch.sum(prob * simStruc, dim=2)\n",
    "\n",
    "        return simClass\n",
    "\n",
    "\n",
    "    def forward_loss(self, x, y):\n",
    "        simClass = self.forward(x)\n",
    "        y = Variable(y)\n",
    "        marginM = torch.zeros(simClass.shape)\n",
    "        marginM[torch.arange(0, marginM.shape[0]), y] = self.margin\n",
    "        lossClassify = F.cross_entropy(self.la * (simClass - marginM), y)\n",
    "        if self.tau > 0 and self.K > 1:\n",
    "            simCenter = centers.t().matmul(centers)\n",
    "            reg = torch.sum(torch.sqrt(2.0 + 1e-5 - 2. * simCenter[self.weight])) / (self.cN * self.K * (self.K - 1.))\n",
    "            return lossClassify + self.tau * reg\n",
    "        else:\n",
    "            return lossClassify\n",
    "\n",
    "\n",
    "    def train_loop(self, epoch, train_loader, optimizer):\n",
    "        print_freq = 10\n",
    "        avg_loss = 0\n",
    "\n",
    "\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            loss = self.forward_loss(x, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            avg_loss = avg_loss + loss.item()\n",
    "\n",
    "            if i % print_freq == 0:\n",
    "                # print(optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "                print('Epoch {:d} | Batch {:d}/{:d} | Loss {:f}'.format(epoch, i, len(train_loader),\n",
    "                                                                        avg_loss / float(i + 1)))\n",
    "\n",
    "    def test_loop(self, val_loader):\n",
    "        if self.DBval:\n",
    "            return self.analysis_loop(val_loader)\n",
    "        else:\n",
    "            return -1  # no validation, just save model during iteration\n",
    "\n",
    "    def analysis_loop(self, val_loader, record=None):\n",
    "        class_file = {}\n",
    "        for i, (x, y) in enumerate(val_loader):\n",
    "            x = x.cuda()\n",
    "            x_var = Variable(x)\n",
    "            feats = self.feature.forward(x_var).data.cpu().numpy()\n",
    "            labels = y.cpu().numpy()\n",
    "            for f, l in zip(feats, labels):\n",
    "                if l not in class_file.keys():\n",
    "                    class_file[l] = []\n",
    "                class_file[l].append(f)\n",
    "\n",
    "        for cl in class_file:\n",
    "            class_file[cl] = np.array(class_file[cl])\n",
    "\n",
    "        DB = DBindex(class_file)\n",
    "        print('DB index = %4.2f' % (DB))\n",
    "        return 1 / DB  # DB index: the lower the better\n",
    "\n",
    "\n",
    "def DBindex(cl_data_file):\n",
    "    # For the definition Davis Bouldin index (DBindex), see https://en.wikipedia.org/wiki/Davies%E2%80%93Bouldin_index\n",
    "    # DB index present the intra-class variation of the data\n",
    "    # As baseline/baseline++ do not train few-shot classifier in training, this is an alternative metric to evaluate the validation set\n",
    "    # Emperically, this only works for CUB dataset but not for miniImagenet dataset\n",
    "\n",
    "    class_list = cl_data_file.keys()\n",
    "    cl_num = len(class_list)\n",
    "    cl_means = []\n",
    "    stds = []\n",
    "    DBs = []\n",
    "    for cl in class_list:\n",
    "        cl_means.append(np.mean(cl_data_file[cl], axis=0))\n",
    "        stds.append(np.sqrt(np.mean(np.sum(np.square(cl_data_file[cl] - cl_means[-1]), axis=1))))\n",
    "\n",
    "    mu_i = np.tile(np.expand_dims(np.array(cl_means), axis=0), (len(class_list), 1, 1))\n",
    "    mu_j = np.transpose(mu_i, (1, 0, 2))\n",
    "    mdists = np.sqrt(np.sum(np.square(mu_i - mu_j), axis=2))\n",
    "\n",
    "    for i in range(cl_num):\n",
    "        DBs.append(np.max([(stds[i] + stds[j]) / mdists[i, j] for j in range(cl_num) if j != i]))\n",
    "    return np.mean(DBs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model           = BaselineTrainSoft( model_dict['Conv4'], la = 20, gamma = 0.1, tau=0.2, margin=0.01, K=10, cN=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0125,  0.0220,  0.0003,  ..., -0.0117, -0.0030, -0.0176],\n",
       "        [ 0.0073, -0.0082,  0.0056,  ...,  0.0091,  0.0127,  0.0139],\n",
       "        [ 0.0043,  0.0114, -0.0197,  ..., -0.0193, -0.0005, -0.0089],\n",
       "        ...,\n",
       "        [-0.0006, -0.0164, -0.0019,  ...,  0.0221, -0.0036, -0.0177],\n",
       "        [ 0.0155,  0.0201, -0.0022,  ...,  0.0011,  0.0204, -0.0114],\n",
       "        [-0.0004,  0.0104,  0.0202,  ...,  0.0049,  0.0021, -0.0048]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of BaselineTrainSoft(\n",
       "  (feature): ConvNet(\n",
       "    (trunk): Sequential(\n",
       "      (0): ConvBlock(\n",
       "        (C): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (BN): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (trunk): Sequential(\n",
       "          (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "      )\n",
       "      (1): ConvBlock(\n",
       "        (C): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (BN): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (trunk): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "      )\n",
       "      (2): ConvBlock(\n",
       "        (C): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (BN): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (trunk): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "      )\n",
       "      (3): ConvBlock(\n",
       "        (C): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (BN): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (trunk): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Flatten()\n",
       "    )\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fc',\n",
       " 'feature.trunk.0.BN.bias',\n",
       " 'feature.trunk.0.BN.num_batches_tracked',\n",
       " 'feature.trunk.0.BN.running_mean',\n",
       " 'feature.trunk.0.BN.running_var',\n",
       " 'feature.trunk.0.BN.weight',\n",
       " 'feature.trunk.0.C.bias',\n",
       " 'feature.trunk.0.C.weight',\n",
       " 'feature.trunk.0.trunk.0.bias',\n",
       " 'feature.trunk.0.trunk.0.weight',\n",
       " 'feature.trunk.0.trunk.1.bias',\n",
       " 'feature.trunk.0.trunk.1.num_batches_tracked',\n",
       " 'feature.trunk.0.trunk.1.running_mean',\n",
       " 'feature.trunk.0.trunk.1.running_var',\n",
       " 'feature.trunk.0.trunk.1.weight',\n",
       " 'feature.trunk.1.BN.bias',\n",
       " 'feature.trunk.1.BN.num_batches_tracked',\n",
       " 'feature.trunk.1.BN.running_mean',\n",
       " 'feature.trunk.1.BN.running_var',\n",
       " 'feature.trunk.1.BN.weight',\n",
       " 'feature.trunk.1.C.bias',\n",
       " 'feature.trunk.1.C.weight',\n",
       " 'feature.trunk.1.trunk.0.bias',\n",
       " 'feature.trunk.1.trunk.0.weight',\n",
       " 'feature.trunk.1.trunk.1.bias',\n",
       " 'feature.trunk.1.trunk.1.num_batches_tracked',\n",
       " 'feature.trunk.1.trunk.1.running_mean',\n",
       " 'feature.trunk.1.trunk.1.running_var',\n",
       " 'feature.trunk.1.trunk.1.weight',\n",
       " 'feature.trunk.2.BN.bias',\n",
       " 'feature.trunk.2.BN.num_batches_tracked',\n",
       " 'feature.trunk.2.BN.running_mean',\n",
       " 'feature.trunk.2.BN.running_var',\n",
       " 'feature.trunk.2.BN.weight',\n",
       " 'feature.trunk.2.C.bias',\n",
       " 'feature.trunk.2.C.weight',\n",
       " 'feature.trunk.2.trunk.0.bias',\n",
       " 'feature.trunk.2.trunk.0.weight',\n",
       " 'feature.trunk.2.trunk.1.bias',\n",
       " 'feature.trunk.2.trunk.1.num_batches_tracked',\n",
       " 'feature.trunk.2.trunk.1.running_mean',\n",
       " 'feature.trunk.2.trunk.1.running_var',\n",
       " 'feature.trunk.2.trunk.1.weight',\n",
       " 'feature.trunk.3.BN.bias',\n",
       " 'feature.trunk.3.BN.num_batches_tracked',\n",
       " 'feature.trunk.3.BN.running_mean',\n",
       " 'feature.trunk.3.BN.running_var',\n",
       " 'feature.trunk.3.BN.weight',\n",
       " 'feature.trunk.3.C.bias',\n",
       " 'feature.trunk.3.C.weight',\n",
       " 'feature.trunk.3.trunk.0.bias',\n",
       " 'feature.trunk.3.trunk.0.weight',\n",
       " 'feature.trunk.3.trunk.1.bias',\n",
       " 'feature.trunk.3.trunk.1.num_batches_tracked',\n",
       " 'feature.trunk.3.trunk.1.running_mean',\n",
       " 'feature.trunk.3.trunk.1.running_var',\n",
       " 'feature.trunk.3.trunk.1.weight'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftTriple(nn.Module):\n",
    "    def __init__(self, la, gamma, tau, margin, dim, cN, K):\n",
    "        super(SoftTriple, self).__init__()\n",
    "        self.la = la\n",
    "        self.gamma = 1./gamma\n",
    "        self.tau = tau\n",
    "        self.margin = margin\n",
    "        self.cN = cN\n",
    "        self.K = K\n",
    "        self.fc = Parameter(torch.Tensor(dim, cN*K))\n",
    "        self.weight = torch.zeros(cN*K, cN*K, dtype=torch.bool)\n",
    "        for i in range(0, cN):\n",
    "            for j in range(0, K):\n",
    "                self.weight[i*K+j, i*K+j+1:(i+1)*K] = 1\n",
    "        init.kaiming_uniform_(self.fc, a=math.sqrt(5))\n",
    "        return\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        centers = F.normalize(self.fc, p=2, dim=0)\n",
    "        simInd = input.matmul(centers)\n",
    "        simStruc = simInd.reshape(-1, self.cN, self.K)\n",
    "        prob = F.softmax(simStruc*self.gamma, dim=2)\n",
    "        simClass = torch.sum(prob*simStruc, dim=2)\n",
    "        marginM = torch.zeros(simClass.shape)\n",
    "        marginM[torch.arange(0, marginM.shape[0]), target] = self.margin\n",
    "        lossClassify = F.cross_entropy(self.la*(simClass-marginM), target)\n",
    "        if self.tau > 0 and self.K > 1:\n",
    "            simCenter = centers.t().matmul(centers)\n",
    "            reg = torch.sum(torch.sqrt(2.0+1e-5-2.*simCenter[self.weight]))/(self.cN*self.K*(self.K-1.))\n",
    "            return lossClassify+self.tau*reg\n",
    "        else:\n",
    "            return lossClassify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = SoftTriple(la = 20, gamma = 0.1, tau=0.2, margin=0.01, dim=1800, cN=200, K=10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('fc',\n",
       "              tensor([[ 0.0061, -0.0220,  0.0207,  ..., -0.0223,  0.0048, -0.0074],\n",
       "                      [-0.0124,  0.0143, -0.0033,  ..., -0.0067,  0.0220,  0.0181],\n",
       "                      [-0.0003, -0.0049,  0.0126,  ..., -0.0083,  0.0203, -0.0079],\n",
       "                      ...,\n",
       "                      [ 0.0223, -0.0216,  0.0189,  ..., -0.0210,  0.0161,  0.0045],\n",
       "                      [ 0.0209, -0.0169,  0.0019,  ..., -0.0138,  0.0115,  0.0037],\n",
       "                      [ 0.0211,  0.0062, -0.0125,  ...,  0.0066,  0.0051,  0.0194]]))])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(5,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0264, 0.0443, 0.1060, 0.2311, 0.1519, 0.0253, 0.0555, 0.0742, 0.1948,\n",
       "         0.0906],\n",
       "        [0.1676, 0.0251, 0.1073, 0.0132, 0.1444, 0.0247, 0.1127, 0.1685, 0.0982,\n",
       "         0.1383],\n",
       "        [0.0637, 0.1627, 0.0166, 0.0180, 0.0240, 0.4840, 0.1047, 0.0611, 0.0180,\n",
       "         0.0471],\n",
       "        [0.0108, 0.1580, 0.0467, 0.0117, 0.0884, 0.0286, 0.0510, 0.2217, 0.1797,\n",
       "         0.2034],\n",
       "        [0.1166, 0.0602, 0.1243, 0.0632, 0.0205, 0.0162, 0.3391, 0.0831, 0.1275,\n",
       "         0.0494]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = F.softmax(a, dim=1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
